---
layout: page
title: About
permalink: /about/
---

I am a computer science student at KIT in Karlsruhe, Germany.
I am interested in [Java](https://en.wikipedia.org/wiki/Java_(programming_language)) and [Rust](https://en.wikipedia.org/wiki/Rust_(programming_language)) programming and general [Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing).

As a hobby I take part in playing [CTF](https://ctftime.org/ctf-wtf/) together with the teams [KITCTF](https://kitctf.de/) and [Eat, Sleep, Pwn, Repeat](https://twitter.com/EatSleepPwnRpt).

### Published Work
**DeNom: a tool to find problematic nominalizations using NLP**

*by Mathias Landhäußer, Sven J. Körner, Walter F. Tichy, Jan Keim, Jennifer Krisch*

Published in: 2015 IEEE Second International Workshop on Artificial Intelligence for Requirements Engineering (AIRE)

**Abstract:**

Nominalizations in natural language requirements specifications can lead to imprecision. For example, in the phrase "transportation of pallets" it is unclear who transports the pallets from where to where and how. Guidelines for requirements specifications therefore recommend avoiding nominalizations. However, not all nominalizations are problematic. We present an industrial-strength text analysis tool called DeNom, which detects problematic nominalizations and reports them to the user for reformulation. DeNom uses Stanford's parser and the Cyc ontology. It classifies nominalizations as problematic or acceptable by first detecting all nominalizations in the specification and then subtracting those which are sufficiently specified within the sentence through word references, attributes, nominal phrase constructions, etc. All remaining nominalizations are incompletely specified, and are therefore prone to conceal complex processes. These nominalizations are deemed problematic. A thorough evaluation used 10 real-world requirements specifications from Daimler AG consisting of 60,000 words. DeNom identified over 1,100 nominalizations and classified 129 of them as problematic. Only 45 of which were false positives, resulting in a precision of 66%. Recall was 88%. In contrast, a naive nominalization detector would overload the user with 1,100 warnings, a thousand of which would be false positives.


[Link](https://doi.org/10.1109/AIRE.2015.7337623)

### Contact me

[jan.keim@protonmail.com](mailto:jan.keim@protonmail.com)
